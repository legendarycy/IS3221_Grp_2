{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c843c6-35db-494f-8044-72ec053251b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import norm, zscore\n",
    "\n",
    "#turn off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18e731",
   "metadata": {},
   "source": [
    "## Part 1. Data Cleaning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038da8d4-416f-434f-84a1-0fcca1e9ab5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading the data\n",
    "\n",
    "df = pd.read_excel('Census-mod.xlsx')\n",
    "\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679817b-d2d9-42a3-a0f4-44a3aaf81732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining data types of each column\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278000e1-903e-485e-934c-58f8b6f59ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many rows and columns this dataset has\n",
    "\n",
    "print(\"Number of rows : \", df.shape[0])\n",
    " \n",
    "# Obtaining the number of columns\n",
    "print(\"Number of columns : \", df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c4115a",
   "metadata": {},
   "source": [
    "### 1.1. Convert Columns to Correct Data Type ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cdbe45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the categorical columns to a categorical datatype without numeric encoding\n",
    "cat_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'income']\n",
    "for column in cat_columns:\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "# Verify the change by displaying the datatypes again\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5a395",
   "metadata": {},
   "source": [
    "### 1.2. Removal of Duplicates ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f187fd9-c729-49b3-b683-93cbc8159e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabulate and identify if there any duplicate records (not duplicate cells) in which case you need to remove them. \n",
    "# Identify these duplicates in your report. \n",
    "\n",
    "pd.DataFrame(df)\n",
    "duplicate_records = df[df.duplicated(keep=False)]\n",
    "\n",
    "if not duplicate_records.empty:\n",
    "    print(\"\\nDuplicate Records:\")\n",
    "else:\n",
    "    print(\"\\nNo Duplicate Records Found.\")\n",
    "\n",
    "display(duplicate_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2523feb5-3d82-44b7-9b15-82c47db062b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated records\n",
    "\n",
    "census = df.drop_duplicates(keep='first')\n",
    "\n",
    "print(\"\\nDataFrame after removing duplicates:\")\n",
    "display(census)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4eabf",
   "metadata": {},
   "source": [
    "### 1.3. Fix Spelling Errors ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4596e-2ea9-4090-8ae7-13b6b5b69f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying unique values across categorical variables\n",
    "cat_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'income']\n",
    "\n",
    "unique_values_list = []\n",
    "\n",
    "# Iterate through columns of interest and populate the list with unique values and column names\n",
    "for column in cat_columns:\n",
    "    unique_values = sorted(census[column].unique())\n",
    "    for value in unique_values:\n",
    "        unique_values_list.append([column, value])\n",
    "\n",
    "# Create a DataFrame from the list with appropriate column names\n",
    "unique_values_df = pd.DataFrame(unique_values_list, columns=['Column_Name', 'Unique_Value'])\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(\"DataFrame with column names and unique values:\")\n",
    "display(unique_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6bae4-6640-4791-b0c5-1f9d70e54b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency of spelling within categorical variables\n",
    "\n",
    "# marital-status\n",
    "census.loc[:, 'marital-status'] = census['marital-status'].str.replace('married-AF-spouse', 'Married-AF-spouse')\n",
    "\n",
    "# relationship\n",
    "census.loc[:, 'relationship'] = census['relationship'].str.replace('husband', 'Husband')\n",
    "census.loc[:, 'relationship'] = census['relationship'].str.replace('wife', 'Wife')\n",
    "\n",
    "# sex\n",
    "census.loc[:, 'sex'] = census['sex'].str.replace('Mole', 'Male')\n",
    "census.loc[:, 'sex'] = census['sex'].str.replace('Femole', 'Female')\n",
    "\n",
    "# native-country \n",
    "census.loc[:, 'native-country'] = census['native-country'].str.replace('Hong', 'Hong-Kong')\n",
    "census.loc[:, 'native-country'] = census['native-country'].str.replace('South', 'South-Korea')\n",
    "census.loc[:, 'native-country'] = census['native-country'].str.replace('Holand-Netherlands', 'Holland-Netherlands')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a9f4bd",
   "metadata": {},
   "source": [
    "### 1.4. Imputing Missing Data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0f22f",
   "metadata": {},
   "source": [
    "Categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a88138-8203-455b-9dfb-f9bd4b8338f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values \"?\" within categorical variables\n",
    "\n",
    "for column in cat_columns:\n",
    "    census.loc[:, column] = census[column].str.replace('?', 'Others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c184f7-f28d-431b-8d9f-51a411e42237",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check data ##\n",
    "cleaned_unique = []\n",
    "\n",
    "# Iterate through columns of interest and populate the list with unique values and column names\n",
    "for column in cat_columns:\n",
    "    unique_values = sorted(census[column].unique())\n",
    "    for value in unique_values:\n",
    "        cleaned_unique.append([column, value])\n",
    "\n",
    "# Create a DataFrame from the list with appropriate column names\n",
    "unique_df = pd.DataFrame(cleaned_unique, columns=['Column_Name', 'Unique_Value'])\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(\"DataFrame with column names and unique values:\")\n",
    "display(unique_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b5d33e",
   "metadata": {},
   "source": [
    "Numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51839c3-00ed-4b8c-af18-c5e514a79fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in numerical columns \n",
    "num_columns = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "missing_values = []\n",
    "\n",
    "# Loop through numerical columns and check for missing values (represented by 99999)\n",
    "for column in num_columns:\n",
    "    if (census[column] == 99999).any():\n",
    "        missing_values.append(column)\n",
    "\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25f4b1-8223-4f28-a975-b419c058b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect missing values in capital-gain\n",
    "missing = census['capital-gain'] == 99999\n",
    "missing_indices = census[missing].index\n",
    "\n",
    "# Original\n",
    "census[missing]\n",
    "\n",
    "# Suggested (to save space)\n",
    "print(f\"Number of rows with missing capital gain: {len(census[missing])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e1115-726f-4f62-b072-d9eef6a37e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values within capital-gain have income >50k, examine distribution of >50k and <= 50K\n",
    "\n",
    "income_groups = ['>50K', '<=50K']\n",
    "\n",
    "# Create subplots to visualize the normal distribution of 'capital-gain' for each income group\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
    "\n",
    "for i, group in enumerate(income_groups):\n",
    "    # Filter data for the current income group\n",
    "    group_data = census[census['income'] == group]['capital-gain']\n",
    "    \n",
    "    # Calculate mean and standard deviation of 'capital-gain' for the current group\n",
    "    mu, std = group_data.mean(), group_data.std()\n",
    "    \n",
    "    # Generate values for the x-axis\n",
    "    x = np.linspace(mu - 3*std, mu + 3*std, 100)\n",
    "    \n",
    "    # Calculate the PDF (Probability Density Function) for the normal distribution\n",
    "    pdf = norm.pdf(x, mu, std)\n",
    "    \n",
    "    # Plot the normal distribution curve\n",
    "    axes[i].plot(x, pdf, 'r-', label='Normal Distribution')\n",
    "    axes[i].hist(group_data, bins=10, density=True, alpha=0.7, label='Data Distribution')\n",
    "    axes[i].set_title(f'Normal Distribution of Capital Gain for {group} Income')\n",
    "    axes[i].set_xlabel('Capital Gain')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402be4d9-d306-4e07-b41e-c9bc1e580794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of 'capital-gain' for the income group '>50K' excluding values of 99999\n",
    "mean_capgain = census[(census['income'] == '>50K') & (census['capital-gain'] != 99999)]['capital-gain'].mean()\n",
    "\n",
    "# Replace missing values (99999) in 'capital-gain' for income group '>50K' with the calculated mean\n",
    "census.loc[(census['income'] == '>50K') & (census['capital-gain'] == 99999), 'capital-gain'] = mean_capgain\n",
    "\n",
    "(census['capital-gain'] == 99999).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8aedf5-6f87-4331-96b3-b18c0441b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_groups = ['>50K', '<=50K']\n",
    "\n",
    "# Create subplots to visualize the normal distribution of 'capital-gain' for each income group\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
    "\n",
    "for i, group in enumerate(income_groups):\n",
    "    # Filter data for the current income group\n",
    "    group_data = census[census['income'] == group]['capital-gain']\n",
    "    \n",
    "    # Calculate mean and standard deviation of 'capital-gain' for the current group\n",
    "    mu, std = group_data.mean(), group_data.std()\n",
    "    \n",
    "    # Generate values for the x-axis\n",
    "    x = np.linspace(mu - 3*std, mu + 3*std, 100)\n",
    "    \n",
    "    # Calculate the PDF (Probability Density Function) for the normal distribution\n",
    "    pdf = norm.pdf(x, mu, std)\n",
    "    \n",
    "    # Plot the normal distribution curve\n",
    "    axes[i].plot(x, pdf, 'r-', label='Normal Distribution')\n",
    "    axes[i].hist(group_data, bins=10, density=True, alpha=0.7, label='Data Distribution')\n",
    "    axes[i].set_title(f'Normal Distribution of Capital Gain for {group} Income')\n",
    "    axes[i].set_xlabel('Capital Gain')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1285cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c9999df",
   "metadata": {},
   "source": [
    "### 2. Correlation between Variables ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatter matrix\n",
    "pd.plotting.scatter_matrix(census, alpha=0.5, figsize=(20, 12), diagonal='hist')\n",
    "# Turn off x and y axis ticks for all subplots for greater clarity\n",
    "for ax in plt.gcf().get_axes():\n",
    "    ax.tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1047be51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate correlation\n",
    "correlation_matrix = census.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc8bef",
   "metadata": {},
   "source": [
    "Observations from the scatter matrix:  \n",
    "* Most of the scatter plots do not show a clear linear relationship, suggesting that these variables are relatively independent of each other.  \n",
    "* Variables like 'capital-gain' and 'capital-loss' have many 0 values, which makes sense since not everyone has capital transactions.  \n",
    "\n",
    "Observations from the correlation matrix:  \n",
    "* None of the variables have a correlation coefficient close to 1 or -1 (the highest absolute correlation coefficient is 0.147), which suggests that there is little collinearity between any two variables.\n",
    "* 'age' has a slight positive correlation with 'capital-gain' and 'hours-per-week'. This means that as an individual's age increases, their capital gain and hours worked per week tend to increase slightly.  \n",
    "* 'education-num' has a positive correlation with 'capital-gain' and 'hours-per-week'. This means that individuals with higher education levels may have higher capital gains and work more hours per week.  \n",
    "* 'capital-gain' and 'capital-loss' have a slight negative correlation, so when one increases, the other tends to decrease.  \n",
    "\n",
    "**Are all variables necessary for analysis?**  \n",
    "Yes. As multicollinearity is not observed, all variables provide insights on and influence the target variable 'income'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499df236",
   "metadata": {},
   "source": [
    "### 3. Outlier Detection\n",
    "\n",
    "We detected outliers using seaborn's boxplot; for each variable, data points which lie above the third quartile plus 1.5 times the inter-quartile range are determined to be outliers. To better visualize the data, we normalized them using z-score. \n",
    "\n",
    "#### observations\n",
    "\n",
    "From the diagram below, we observed that the outliers for capital-gain and capital-loss above the 3rd quartile, and exhibit high degrees of variability. On average, the proportion of outliers for both features is approximately 4.7%. Further analysis of the distributions revealed that these two metrics are extremely left-skewed, with most people having capital gains and losses of 0. Taken together, this suggests that most people do not invest, and that the performance of those who do vary significantly.\n",
    "\n",
    "We also observed that the hours-per-week variable has outliers beyond both the 1st and 3rd quartiles, with a moderate level of variability - in total, they make up 7.9% of the data points. From the histogram, we also observed that the mode lies at approximately 40 hours per week and that the distribution is relatively light-tailed on both extremes. This means that while there are individuals who work significantly shorter or longer hours, working hours of the population as a whole remain rather consistent.\n",
    "\n",
    "Lastly, there are also a large number of outliers for the age variable, but the variability is relatively smaller; the dataset consists of 8.31% outliers for age. The mean age within the population is 38.6, with a standard deviation of 13.7. The histogram displays a left-skewed distribution, indicating that there are a substantial number of young people within the population. This is a positive sign, as a left-skewed age distribution often suggests a higher proportion of individuals in their prime working years, contributing to economic productivity and vitality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show outliers using boxplot (only for numeric variables since boxplots can only capture outliers based on standard deviation)\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Normalize data using zscore\n",
    "sns.boxplot(data=zscore(census[[i for i in num_columns if i != \"education-num\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54302d8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a column for each numeric variable indicating if a row is an outlier for the given variable\n",
    "# Based on |zscore| threshold of 1.5, in line with boxplot thresholds\n",
    "\n",
    "data = []\n",
    "filtered_num_cols = [i for i in num_columns if i not in ['education-num', 'fnlwgt']]\n",
    "\n",
    "for c in filtered_num_cols:\n",
    "    zs = zscore(census[c])\n",
    "    df[f\"is_outlier_{c}\"] =  zs > 1.5\n",
    "    outlier_prop = len(zs[zs>1.5])/len(census)*100\n",
    "    data.append(outlier_prop)\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.bar(filtered_num_cols, data)\n",
    "for i, v in enumerate(data):\n",
    "    plt.text(i, v+0.1, f\"{round(v, 2)}%\", ha='center')\n",
    "plt.xlabel('Numeric Variables')\n",
    "plt.ylabel('Percentage of Outliers')\n",
    "plt.title('Percentage of Outliers in Numeric Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc01602",
   "metadata": {},
   "source": [
    "As the threshold of |z-score| > 1.5 is relatively conservative, a decent number of data points have been identified as outliers.  \n",
    "\n",
    "Observations:  \n",
    "* The percentage of outliers for variables like 'age', 'education-num', and 'hours-per-week' is relatively high. However, it does not make sense to ignore these variables based solely on their outlier percentages, as they provide valuable insights into the target variable 'income'.  \n",
    "* Variables like capital-gain and capital-loss inherently have a high concentration of values at 0, meaning many individuals did not have any capital transactions. The outliers in these variables might be significant events or transactions that could be of interest.  \n",
    "* Some outliers may be realistic when we consider the context. For instance, while working 80 hours a week is statistically an outlier, it might be a valid data point in specific industries or professions.  \n",
    "\n",
    "**Should any of the variables be ignored?**  \n",
    "No, as the outliers percentage is high due to the nature of the variables as explained above. Additionally, the variables all provide insight to the target variable, 'income'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e4b54",
   "metadata": {},
   "source": [
    "### 4. Frequency/Density Distribution for Each Feature ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995432c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Initialize a dictionary to store the computed statistics for each variable\n",
    "stats_dict = {}\n",
    "\n",
    "# Iterate over numerical columns to compute the statistics and generate the plots\n",
    "for col in [i for i in num_columns if i not in ['education-num', 'fnlwgt']]:\n",
    "    # Plotting the frequency/density distribution\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    sns.histplot(census[col], stat='density', kde=True, bins=30)\n",
    "    plt.title(f\"Frequency Density Distribution of {col}\")\n",
    "    plt.ylabel('Density')\n",
    "    plt.xlabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {}\n",
    "    stats['Mode'] = census[col].mode()[0]\n",
    "    stats['Median'] = census[col].median()\n",
    "    stats['Mean'] = census[col].mean()\n",
    "    stats['Variance'] = census[col].var()\n",
    "    stats['Std. Dev.'] = census[col].std()\n",
    "    stats['Skewness'] = skew(census[col])\n",
    "    stats['Kurtosis'] = kurtosis(census[col])\n",
    "    \n",
    "    stats_dict[col] = stats\n",
    "    \n",
    "    # Print statistics under each graph\n",
    "    print(\"--- Central Tendency ---\")\n",
    "    for key,value in list(stats.items())[:3]:\n",
    "        print(key, ':', value)\n",
    "        \n",
    "    print(\"--- Variation ---\")\n",
    "    for key,value in list(stats.items())[3:5]:\n",
    "        print(key, ':', value)\n",
    "    \n",
    "    print(\"--- Shape ---\")\n",
    "    for key,value in list(stats.items())[5:]:\n",
    "        print(key, ':', value)    \n",
    "\n",
    "# Print stats in a dataframe for easy comparison\n",
    "stats_df = pd.DataFrame(stats_dict).T\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c14766",
   "metadata": {},
   "source": [
    "**Observations/Comments:**  \n",
    "\n",
    "age  \n",
    "* The graph shows the distribution of the number of individuals of a certain age.  \n",
    "* There is a peak around 20-40 years.  \n",
    "* The mean age is approximately 38.65 years.  \n",
    "* A positive skewness of 0.557 suggests a right-skewed distribution.  * A negative kurtosis value of -0.187 suggests a light tail with  less extreme large jumps away from the mean than a normal distribution.  \n",
    "\n",
    "education-num:  \n",
    "* The graph shows the distribution of the number of individuals of a certain education level.  \n",
    "* There is a peak around 9 and 13, corresponding to high school and bachelor's degrees respectively.  \n",
    "* The mean education level is around 10.  \n",
    "* The skewness value of -0.314 is above -0.5, which means that the distribution is fairly symmetrical.  \n",
    "* A positive kurtosis value of 0.621 suggests heavy tails with more frequent large jumps away from the mean than a normal distribution.  \n",
    "\n",
    "capital-gain:  \n",
    "* The graph shows the distribution of the number of individuals of a certain capital gain.  \n",
    "* There is a peak at 0, which suggests that the majority of individuals have no capital gains.  \n",
    "* The mean capital gain is around 590, but the high standard deviation indicates a wide spread.  \n",
    "* The high positive skewness value of 5.91 suggests a very right-skewed distribution.  \n",
    "* The high positive kurtosis value of 43.5 suggests heavy tails.  \n",
    "\n",
    "capital-loss:  \n",
    "* The graph shows the distribution of the number of individuals of a certain capital loss.  \n",
    "* There is a peak at 0, which suggests that the majority of individuals have no capital losses.  \n",
    "* The mean capital loss is around 87.6.  \n",
    "* The high positive skewness value of 5.47 suggests a very right-skewed distribution.  \n",
    "* The high positive kurtosis value of 20.0 suggests heavy tails.   \n",
    "\n",
    "hours-per-week:  \n",
    "* The graph shows the distribution of the number of individuals of a certain number of working hours per week.  \n",
    "* There is a significant peak at 40 hours, which aligns with the standard workweek.  \n",
    "* The mean is around 40.43 hours, closely matching the mode.  \n",
    "* The skewness value of 0.240 is below 0.5, which means that the distribution is fairly symmetrical.  \n",
    "* The positive kurtosis value of 2.95 suggests heavy tails, with some individuals working significantly more than 40 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb208a",
   "metadata": {},
   "source": [
    "### 5. Frequency Distribution for Each Feature by Label Classes ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56195e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate dataframes based on y variable\n",
    "gt_50 = census.loc[census[\"income\"] == \">50K\",:]\n",
    "lte_50 = census.loc[census[\"income\"] == \"<=50K\",:]\n",
    "\n",
    "fig, ax = plt.subplots(13, 2, figsize = (15, 50))\n",
    "all_cols = [c for c in [*num_columns, *cat_columns] if c != \"income\"]\n",
    "\n",
    "for i in range(0, len(all_cols)):\n",
    "    #calculate col and row number for subplot\n",
    "    col_name = all_cols[i-1]\n",
    "\n",
    "    #plot into the designated cell\n",
    "    ax[i, 0].hist(census[col_name], color = \"orange\")\n",
    "    ax[i, 0].hist(gt_50[col_name], color = \"green\", alpha = 0.7, label = \">50K\")\n",
    "    ax[i, 0].set_title(f\"Variable: {col_name} (Income > $50K)\")\n",
    "\n",
    "    ax[i, 1].hist(census[col_name], color = \"orange\")\n",
    "    ax[i, 1].hist(lte_50[col_name], color = \"red\", alpha = 0.7, label = \"<=50K\")\n",
    "    ax[i, 1].set_title(f\"Variable: {col_name} (Income <= $50K)\")\n",
    "\n",
    "    ax[i, 0].set_ylabel(\"Frequency\")\n",
    "    ax[i, 1].set_ylabel(\"Frequency\")\n",
    "    \n",
    "    #show legend based on labels\n",
    "    ax[i, 0].legend()\n",
    "    ax[i, 1].legend()\n",
    "\n",
    "    #rotate x tick label if it's a categorical variable\n",
    "    if col_name in cat_columns:\n",
    "        for tick in ax[i, 0].get_xticklabels():\n",
    "            tick.set_rotation(90)\n",
    "        for tick in ax[i, 1].get_xticklabels():\n",
    "            tick.set_rotation(90)\n",
    "\n",
    "#set spacing\n",
    "plt.subplots_adjust(left=-0.2, bottom=0, right=0.85, top=1.5, wspace=0.1, hspace=1.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fcf65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['age', 'hours-per-week', 'education-num']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, len(selected_cols), figsize = (16, 8))\n",
    "\n",
    "for i, col_name in enumerate(selected_cols):\n",
    "    sns.boxplot(data=census, x='income', y=col_name, ax=ax[i], palette={\"<=50K\": \"blue\", \">50K\": \"orange\"})    \n",
    "    ax[i].set_title(f\"Plot of {col_name} for Different Income Groups\")\n",
    "    ax[i].set_xlabel(\"Income\")\n",
    "    ax[i].set_ylabel(col_name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e4a542",
   "metadata": {},
   "source": [
    "**Frequency Distribution Observations:**  \n",
    "The frequency distribution graphs show the distribution of the stated variable in blue, grouped by income >50K on the left and <=50K on the right.  \n",
    "\n",
    "**<font color='red'>please help check and supplement the observations cos idk what more to say</font>**  \n",
    "\n",
    "native-country:  \n",
    "<font color='red'>idk what to say</font>  \n",
    "\n",
    "age:  \n",
    "* People earning more than 50K generally tend to be older, with the median age being higher than that of the <=50K group. This might indicate experience and seniority.  \n",
    "\n",
    "education-num and education:  \n",
    "* Higher education levels (like Bachelors, Masters, and Doctorate) have a relatively higher representation in the >50K group.  \n",
    "* HS-grad and some college education dominate the <=50K group.  \n",
    "\n",
    "capital-gain and capital-loss:  \n",
    "* Most individuals with a low capital gain/loss or a capital gain/loss of 0 have income <=50K.  \n",
    "* This suggests that individuals with higher income have more means to invest in capital and make capital gains/losses.  \n",
    "\n",
    "hours-per-week:  \n",
    "* Those earning more than 50K tend to work longer hours, with a notable number of individuals working more than 40 hours a week.  \n",
    "* For those earning 50K or less, the majority work around 40 hours, but there is also a significant portion working less than that.  \n",
    "\n",
    "work-class:  \n",
    "* For both individuals earning >50K and <=50K, most of them fall under the private work class.  \n",
    "\n",
    "marital-status:  \n",
    "* For individuals earning >50K, most of them fall under the Married-civ-spouse status.  \n",
    "* For individuals earning <=50K, most of them fall under the Never-married and Married-civ-spouse statuses.  \n",
    "\n",
    "occupation:  \n",
    "* For individuals earning >50K, most of them fall under exec-managerial roles.  \n",
    "* For individuals earning <=50K, most of them fall under exec-managerial, others and sales roles.  \n",
    "\n",
    "relationship:  \n",
    "* For individuals earning >50K, most of them fall under Husband roles.  \n",
    "* For individuals earning <=50K, most of them fall under Not-in-family roles.  \n",
    "\n",
    "race:  \n",
    "* The majority of individuals in both income groups are White. However, when comparing the proportions, Whites and Asians have a higher representation in the >50K group relative to their total population than other races.  \n",
    "\n",
    "sex:  \n",
    "* A significant number of males earn more than 50K compared to females. Conversely, more females earn 50K or less.  \n",
    "\n",
    "\n",
    "**Box Plot Observations:**  \n",
    "\n",
    "* For age, hours-per-week and education-num, the interquartile range is wider for the >50K group, indicating more variability. Outliers are evident, especially in the hours-per-week distribution for both income groups, suggesting some individuals work exceptionally long or short hours.  \n",
    "* In summary, variables like age, education, and hours-per-week show distinct patterns between the two income groups, which could be vital predictors in modeling income. The differences in gender and race distributions suggest societal or sector-specific income disparities that might be worth exploring further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca773eb",
   "metadata": {},
   "source": [
    "## 6. Predictive Models\n",
    "\n",
    "We then exported the data for further transformation and storyboarding on SAC. After which, we re-imported the data back to python for to generate predictive models. In one-hot encoding the categorical variables, we prioritised the dropping of the 'Others' class. This is relevant to our feature selection process, which we'll discuss further in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028efb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import transformed data, remove fnlwgt, education-num, marital-status, occupation, capital-gain, capital-loss, income\n",
    "#these categorical variables are redundant because there are other columns which conveys the same information as them or have already been aggregated into other columns. \n",
    "\n",
    "#for instance, education indicates the highest education level attained and education-num indicates the number of years spent studying. these two will be correlated\n",
    "#since the longer you spend studying (typically), then the higher your education level should be.\n",
    "\n",
    "\n",
    "#function to one-hot encode a dataframe of categorical variables\n",
    "def get_dummies_from_df(df):\n",
    "    dummies = pd.DataFrame()\n",
    "    for c in df.columns:\n",
    "        #check if categorical variable has 'Others' class; if yes, drop it  when one-hot encoding\n",
    "        if 'Others' in df[c].unique():\n",
    "            d = pd.get_dummies(df[c], prefix = c)\n",
    "            d = d.drop(f\"{c}_Others\", axis = 1)\n",
    "        #else, drop the first class when one-hot encoding\n",
    "        else:\n",
    "            d = pd.get_dummies(df[c], drop_first = True, prefix = c)\n",
    "\n",
    "        #concatenate dummies column-wise\n",
    "        if dummies.empty:\n",
    "            dummies = d\n",
    "        else:\n",
    "            dummies = pd.concat([dummies, d], axis = 1)\n",
    "        \n",
    "    #return dummies\n",
    "    return dummies\n",
    "\n",
    "redundant_cols = [\"age\", \"workclass\", \"education\", \"education-num\", \"marital-status\", \"relationship\", \"occupation\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"income\", \"fnlwgt\"]\n",
    "\n",
    "df2 = pd.read_csv(\"Census-mod-cleaned-excel-values.csv\").drop(redundant_cols, axis = 1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split into training and testing, one hot encode X\n",
    "X = df2.drop(\"Income-Grp\", axis = 1)\n",
    "X = get_dummies_from_df(X)\n",
    "y = df2[\"Income-Grp\"]\n",
    "\n",
    "#90% to training, 10% to testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8842bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "#function to get class which was dropped during one-hot encoding\n",
    "def get_dropped_class(original_col, original_df, new_df):\n",
    "    original_classes = original_df[original_col].unique()\n",
    "    print(original_classes)\n",
    "    one_hot_encoded_classes = [col.replace(original_col+\"_\", \"\") for col in new_df.columns if original_col in col]\n",
    "    print(one_hot_encoded_classes)\n",
    "    dropped = [c for c in original_classes if c not in one_hot_encoded_classes]\n",
    "    return dropped\n",
    "\n",
    "#function to turn insignificant features into key-value pairs of categorical variable and classes\n",
    "def generate_insignificant_dict(insignificant_features):\n",
    "    dict = {}\n",
    "    cat_vars = set([cls.split(\"_\")[0] for cls in insignificant_features])\n",
    "    \n",
    "    for var in cat_vars:\n",
    "        cat_classes = [i.split(var+\"_\")[1] for i in insignificant_features if var in i]\n",
    "        dict[var] = cat_classes\n",
    "\n",
    "    return dict\n",
    "\n",
    "#function to evaluate model performance\n",
    "def evaluate_model(model):\n",
    "    \n",
    "    #predict and evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_tr = model.predict(X_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred_proba_train = model.predict_proba(X_train)[:, 1] # Use the probabilities for the positive class (class = 1)\n",
    "    y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    accuracy_tr = model.score(X_train, y_train)\n",
    "\n",
    "    # AUC ROC using probabilities\n",
    "    auc_train = roc_auc_score(y_train, y_pred_proba_train)\n",
    "    auc_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "\n",
    "    #weighted f1 since positive cases only make up 23.9% of the dataset; significant data imbalance\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average = \"weighted\")\n",
    "    weighted_f1_tr = f1_score(y_train, y_pred_tr, average = \"weighted\")\n",
    "    #regular f1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_tr = f1_score(y_train, y_pred_tr)\n",
    "\n",
    "    #confusion_matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrix = pd.DataFrame(conf_matrix, index = [0, 1], columns = [0, 1])\n",
    "\n",
    "    #other metrics from cf\n",
    "    cf_vals = conf_matrix.values\n",
    "    tpr = cf_vals[1,1]/(cf_vals[1,0] + cf_vals[1,1])\n",
    "    fpr = cf_vals[0,1]/(cf_vals[0, 1] + cf_vals[0, 0])\n",
    "    tnr = cf_vals[0,0]/(cf_vals[0,0] + cf_vals[0,1])\n",
    "    fnr = cf_vals[0, 1]/(cf_vals[0, 1] + cf_vals[1, 1])\n",
    "    precision = cf_vals[1, 1]/(cf_vals[1, 1] + cf_vals[0, 1])\n",
    "\n",
    "    #output\n",
    "    print(\"=\"*80)\n",
    "    print(f\"training accuracy: {accuracy_tr:.4f} | testing accuracy: {accuracy:.4f}\")\n",
    "    print(f\"training f1: {f1_tr:.4f} | training weighted f1 {weighted_f1_tr:.4f}\")\n",
    "    print(f\"testing f1: {f1:.4f} | testing weighted f1 {weighted_f1:.4f}\")\n",
    "    print(f\"training auc: {auc_train:.4f} | testing auc: {auc_test:.4f}\\n\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"TPR (Recall or Sensitivity): {tpr:.4f}, TNR (Specificity): {tnr:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, FPR: {fpr:.4f}, FNR: {fnr:.4f}\\n\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"confusion matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "\n",
    "    #plot auc roc\n",
    "    fpr_tr, tpr_tr, threshold_tr = roc_curve(y_train, y_pred_proba_train, pos_label=1)\n",
    "    fpr_te, tpr_te,  threshold_te = roc_curve(y_test, y_pred_proba_test, pos_label=1)\n",
    "\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    plt.plot([0, 1], [0, 1], color = 'navy', linestyle = '--')\n",
    "    plt.plot(fpr_tr, tpr_tr, label = \"ROC (training)\")\n",
    "    plt.plot(fpr_te, tpr_te, label = \"ROC (testing)\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"AUC for {type(model)}\")\n",
    "\n",
    "    basic_metrics  = {\n",
    "        \"accuracy_train\": accuracy_tr,\n",
    "        \"auc_train\": auc_train,\n",
    "        \"f1_train\": f1_tr,\n",
    "        \"accuracy_test\": accuracy,\n",
    "        \"auc_test\": auc_test,\n",
    "        \"f1_test\": f1\n",
    "    }\n",
    "\n",
    "    cf_metrics = {\n",
    "        \"tpr\": tpr,\n",
    "        \"fpr\": fpr,\n",
    "        \"tnr\": tnr,\n",
    "        \"fnr\": fnr,\n",
    "        \"precision\": precision\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"cf_metrics\": cf_metrics,\n",
    "        \"basic_metrics\": basic_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1b85e",
   "metadata": {},
   "source": [
    "### 6.1. Feature Selection\n",
    "\n",
    "We first formed a logistic regression model with L1 regularization. The purpose of doing this is to make use of Lasso Regression's ability to identify insignificant features by means of assigning coefficients of 0 to them. From the model output, we can see that there are several redundant one-hot encoded features from the original workclass-grp, native-country and education-grp categorical variables. \n",
    "\n",
    "With these information, we then simplified these columns by grouping these insignificant classes together with the 'Others' class if it exists as a class within the original categorical columns - otherwise, we group them together and create a new class called 'Others'. This reduced the number of one-hot encoded features from 75 to 58, which in turn led to a significant reduction in the complexity of the models we'll create later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f675d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l1 regularization\n",
    "model_lr_l1 = LogisticRegression(penalty = \"l1\", solver = \"liblinear\", random_state = 42).fit(X_train, y_train)\n",
    "\n",
    "#get insignificant features\n",
    "insignificant_features = X_train.columns[model_lr_l1.coef_[0] == 0]\n",
    "\n",
    "#generate dictionary of insignificant classes for each of the original categorical variables\n",
    "insignificant_classes = generate_insignificant_dict(insignificant_features)\n",
    "\n",
    "#output\n",
    "insignificant_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a724871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep copy of df2 as df3 \n",
    "df3 = df2.copy(deep = True)\n",
    "\n",
    "#simplification of the original categorical variables by grouping insignificant classes together under the class 'Others'\n",
    "for k, v in insignificant_classes.items():\n",
    "    df3.loc[df3[k].isin(v), k] = 'Others'\n",
    "\n",
    "#redo train-test-split with df3 (the simplified df with insignificant classes dropped)\n",
    "X = df3.drop(\"Income-Grp\", axis = 1)\n",
    "X = get_dummies_from_df(X)\n",
    "y = df3[\"Income-Grp\"]\n",
    "\n",
    "#90% to training, 10% to testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)\n",
    "\n",
    "#save training and testing datasets for use on SAC (to be consistent)\n",
    "#pd.concat([y_train, X_train], axis=1).to_csv(\"training_set_new.csv\", index=False)\n",
    "#pd.concat([y_test, X_test], axis=1).to_csv(\"testing_set_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b667f2",
   "metadata": {},
   "source": [
    "### 6.2. Logistic Regression\n",
    "\n",
    "The logistic regression model has high accuracy rate both in and out of sample. Its training and testing F1 scores are also quite good, at 0.6426 and 0.6327 respectively. From the ROC curve, we can see that the area under the curve is also very large, indicating that the model can effectively distinguish between data points from the two income groups. Additionally, the difference in in-sample and out-of-sample AUC is also very minimal, which means that the model performs very consistently and generalizes well to unseen data.\n",
    "\n",
    "From the confusion matrix, we can see that the model has a moderate recall of 0.5457. This means that the model managed to correctly identify approximately half of the positive data points. However, the low FPR of 0.0566 means that its positive predictions are very reliable. Conversely, while the model managed to correctly identify most of the negative data points, the false negative rate of 0.2473 means that its negative predictions are far less reliable than its positive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit logistic\n",
    "model_lr = LogisticRegression().fit(X_train, y_train)\n",
    "summary_lr = evaluate_model(model_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf2b4e",
   "metadata": {},
   "source": [
    "### 6.3. Decision Tree Classifier\n",
    "\n",
    "In our decision tree classifier, the training accuracy of 0.8823 is significantly higher than the testing accuracy of 0.8321. This discrepancy can also be observed in the training and testing f1 scores of 0.7265 and 0.6007 respectively. In the ROC graph, the gap between in-sample and out-of-sample ROC is also very significant. Altogether, it suggests that the model is overfitting and does not generalize well to unseen data.\n",
    "\n",
    "From the confusion matrix, we can see that the model's recall is moderately low, at 0.5260. This means that the model only managed to correctly identify approximately half of the positive data points. However, the low FPR of 0.0712 means that its positive predictions are very reliable. Conversely, while the model managed to correctly identify most of the negative data points (specificity of 0.9288), the false negative rate of 0.3000 means that its negative predictions are far less reliable than its positive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508dd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "summary_dt = evaluate_model(model_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896512c3",
   "metadata": {},
   "source": [
    "### 6.4. Random RandomForestClassifier\n",
    "\n",
    "In our random forest classifier, we can see that the accuracy remains high, but the disparity between in-sample and out-of-sample accuracy is almost as high as that of the decision tree classifier's. This disparity is also observed in the training and testing F1 scores, as well as the training and testing AUC values. However, the gap is much smaller than that of the decision tree model, which indicates that the degree of overfitting is smaller in our random forest model.\n",
    "\n",
    "From the confusion matrix, we can see that the model has a moderately low recall of 0.5525. However, the  false positive rate of 0.0723 suggests that the model's positive predictions are very reliable. In comparison, the model managed to correctly identify a much larger portion of the negative data points but the false negative rate of 0.2929 is rather high, which means that its negative predictions are not as reliable as its positive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "summary_rf = evaluate_model(model_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae00c30",
   "metadata": {},
   "source": [
    "### 6.5. XGBoost Clasifier\n",
    "\n",
    "In our XGBoost model, training and testing accuracies remain high at 0.8604 and 0.8471 respectively. Despite having smaller training and testing f1 scores of 0.6760 and 0.6410 respectively, the difference in in and out-of-sample performance is much smaller than the other two tree-based algorithms, and this is further supported by the smaller gap between the training and testing ROC curves.\n",
    "\n",
    "From the confusion matrix, we can see that the recall is relatively higher, at 0.5687. This means that the model correctly identified slightly over half of the positive cases (income > 50k group). However, it comes at a very low FPR of 0.0650, which means that the data points which the model identifies to be from the high income group are very likely to actually belong in that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = XGBClassifier().fit(X_train, y_train)\n",
    "summary_xgb = evaluate_model(model_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce949c6f",
   "metadata": {},
   "source": [
    "### 6.6. Multi-layer Perceptron\n",
    "\n",
    "For our neural network model, we used multi-layer perceptrons, which is a feed-forward architecture. The in-sample accuracy of 0.8646 is a bit larger than the out-of-sample accuracy of 0.8412. While the training f1 score of 0.6753 is very high and indicates greater ability to distinguish between the two income groups, the out-of-sample f1 score is much lower at 0.6123. Further, the gap between the training and testing ROC curves are also quite substantial. This suggests that the degree of overfitting is also relatively high, and the variance in out-of-sample performance may be very large. \n",
    "\n",
    "From the confusion matrix, we can see that the model has a relatively low recall of 0.5226 and low false positive and negative rates of 0.0583 and 0.2609 respectively. This means that the model is not as good at distinguishing data points from the two income groups from each other as the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp = MLPClassifier().fit(X_train, y_train)\n",
    "summary_mlp = evaluate_model(model_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f65c42",
   "metadata": {},
   "source": [
    "### 6.7. Naiive Bayes\n",
    "\n",
    "Our Naiive Bayes model performed the worst. It has a training and testing accuracy of 0.6986 and 0.7010 respectively, which is much lower than the other models'. However, the disparity between training and testing f1 scores of 0.5918 and 0.5926 respectively indicates that the model performance is very consistent. \n",
    "\n",
    "From the confusion matrix, we can also see that the model has a very high sensitivity of 0.9137 - this means that the model is able to correctly identify a large portion of the positive data points (income > 50k group). However, it comes at the cost of a very high false positive rate of 0.3862. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b634af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = GaussianNB().fit(X_train, y_train)\n",
    "summary_nb = evaluate_model(model_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e1bd9",
   "metadata": {},
   "source": [
    "### 6.8 Support Vector Classifier\n",
    "\n",
    "Our support vector classifier took the longest time to train, but the performance is relatively good and consistent. It has a high training and testing accuracy of 0.8497 and 0.8459 respectively, which are very close to each other. Similarly, its training and testing F1 scores of 0.6253 and 0.6071 also demonstrated moderate consistency between in and out-of-sample performance. From the ROC graph, we can see that the gap between the training and testing ROC are also very small, indicating that the model is able to distinguish between data points of the two income groups quite reliably, and also generalizes well to unseen data.\n",
    "\n",
    "However, from its confusion matrix, we can see that its recall value is relatively low, at only 0.4962. This means that the model is comparatively worse at identifying the positive cases. However, its false positive rate of 0.0437 is also much lower, indicating that the model's positive predictions are much more reliable. Like the other models, our SVC model is also better at correctly identifying the negative data points, but with a lower degree of reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svc = SVC(probability=True).fit(X_train, y_train)\n",
    "summary_svc = evaluate_model(model_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your models and metrics data\n",
    "models = [\n",
    "    \"Logistic Regression\", \"Decision Tree\",\n",
    "    \"Random Forest\", \"XGBoost\",\n",
    "    \"MLP\", \"Naiive Bayes\",\n",
    "    \"SVC\"\n",
    "]\n",
    "\n",
    "# Summary of all models' performance (assuming these are dictionaries)\n",
    "df_basic_metrics = [\n",
    "    summary_lr[\"basic_metrics\"],\n",
    "    summary_dt[\"basic_metrics\"],\n",
    "    summary_rf[\"basic_metrics\"],\n",
    "    summary_xgb[\"basic_metrics\"],\n",
    "    summary_mlp[\"basic_metrics\"],\n",
    "    summary_nb[\"basic_metrics\"],\n",
    "    summary_svc[\"basic_metrics\"]\n",
    "]\n",
    "\n",
    "df_cf_metrics = [\n",
    "    summary_lr[\"cf_metrics\"],\n",
    "    summary_dt[\"cf_metrics\"],\n",
    "    summary_rf[\"cf_metrics\"],\n",
    "    summary_xgb[\"cf_metrics\"],\n",
    "    summary_mlp[\"cf_metrics\"],\n",
    "    summary_nb[\"cf_metrics\"],\n",
    "    summary_svc[\"cf_metrics\"]\n",
    "]\n",
    "\n",
    "# Convert lists of dictionaries to DataFrames\n",
    "df_basic_metrics = pd.DataFrame(df_basic_metrics, index=models)\n",
    "df_cf_metrics = pd.DataFrame(df_cf_metrics, index=models)\n",
    "\n",
    "# View basic metrics\n",
    "df_basic_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49607f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View cf metrics\n",
    "df_cf_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
